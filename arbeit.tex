\documentclass[a4paper]{scrreprt}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage[german,linesnumbered,algoruled,longend,vlined]{algorithm2e}
\DontPrintSemicolon
\SetArgSty{}
\SetKw{KwOr}{or}
\SetKw{KwAnd}{and}
\SetKw{KwNot}{not}
\setlength{\algomargin}{3ex}

\usepackage[fixlanguage]{mybabelbib}
% \selectlanguage{ngerman}
\setbibliographyfont{title}{}
\setbibliographyfont{jtitle}{}
\setbibliographyfont{btitle}{\emph}
\setbibliographyfont{stitle}{\emph}
\setbibliographyfont{journal}{\emph}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{graphicx}
\usepackage[a4paper,bookmarks,bookmarksnumbered]{hyperref}
\usepackage[font=small,format=hang,labelfont=bf,figurename=Abb.,tablename=Tab.]{caption}
\usepackage{enumerate}

\newtheorem{satz}{Satz}[chapter]
\newtheorem{lemma}[satz]{Lemma}
\newtheorem{beobachtung}[satz]{Beobachtung}
\newtheorem{folgerung}[satz]{Folgerung}
\newtheorem{korollar}[satz]{Korollar}
\theoremstyle{definition}
\newtheorem{definition}[satz]{Definition}
\newenvironment{beweis}{\begin{proof}}{\end{proof}}

\graphicspath{{abbildungen/}}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Bitte nur ab hier Änderungen vornehmen %%%%%%%%%%%%%%%%%%%%%

\subject{Seminararbeit}
\title{Randomisierte Datenstrukturen - Treaps}
\author{Moritz Beck, Robert McDaniel}
\date{Eingereicht am 24. 06 2015} %TODO? Gehe davon aus, dass wir das am Mittwoch abgeben
\titlehead{Julius-Maximilians-Universität Würzburg\\
Institut für Informatik\\
Lehrstuhl für Informatik I\\
Effiziente Algorithmen und wissensbasierte Systeme}
\publishers{Betreuer:\\
Prof.\ Dr.\ Alexander Wolff\\
Dipl.-Inf.\ Philipp Kindermann}
\maketitle
\tableofcontents

%%%%% hier den Text einfügen

\chapter{Einführung}
\label{sec:intro}

Treaps sind eine baumartige Datenstruktur, bei der das Sortieren der eingefügten Daten auf zwei Arten erfolgt:
Zum Einen werden die Schlüssel wie in einem Binärbaum sortiert, und zum Anderen werden die Knoten durch eine Priorität wie in einem Heap sortiert.
In dieser Arbeit wird insbesondere auf eine Implementierung von Treaps eingegangen, bei der die Priorität zufällig gewählt wird.
Dies bringt einige Vorteile gegenüber anderen Baumarten, wie zum Beispiel eine hohe Wahrscheinlichkeit, einen balancierten Baum zu erhalten, was die Laufzeit von Operationen verbessert.

Im Laufe dieser Arbeit wird in Abschnitt \ref{sec:motivation} zuerst auf die Situation eingegangen, die zum Einsatz von Treaps führt.
Genauer wird in Abschnitt \ref{sec:motivationbasics} der Aufbau der Datenstruktur, die wir erreichen wollen, erklärt, und in Abschnitt \ref{sec:binaryproblems} die Laufzeiten der gewünschten Methoden bei binären Suchbäumen aufgezeigt und auf die Probleme bei Binärbäumen eingegangen.

In Abschnitt \ref{sec:treaps} werden Treaps als eine Lösung für diese Probleme dargestellt.
Genauer werden in Abschnitt \ref{sec:treapsbasics} die Grundlagen für den Aufbau von Treaps aufgezeigt, in Abschnitt \ref{sec:uniquetreaps} die Existenz eines eindeutigen Treaps für eine gegebene Menge an Paaren von Schlüsseln und Prioritäten bewiesen, was zeigt, dass die Insert-Reihenfolge bei Treaps egal ist. In Abschnitt \ref{sec:implementing} werden die gewünschten Methoden implementiert und in Pseudocode beschrieben.

%TODO Laufzeitanalyse

In Abschnitt \ref{sec:closing} wird ein Abschlussfazit gezogen, wobei noch einmal ein kurzer Vergleich zwischen Binärbäumen und Treaps gemacht wird. %TODO Und vielleicht noch mehr?

\chapter{Motivation}
\label{sec:motivation}

In diesem Kapitel wird der Grundbau der gewünschten Datenstruktur beschrieben und es werden Probleme aufgezeigt, die später mit Treaps gelöst werden.
Hauptsächlich geht es dabei um die Abhängigkeit der Laufzeiten bei Binärbäumen von der Höhe des Binärbaums und das Balancieren dieser Bäume.

\section{Grundmotivation}
\label{sec:motivationbasics}

Wir betrachten dieses grundsätzliche Problem:
Wir wollen eine Menge ${S_1, S_2, ...}$ von Elementen verwalten und dabei bestimmte Methoden für Anfragen und Änderungen unterstützen.
Jedes Element $i$ hat einen Schlüssel $k(i)$, wobei diese Schlüssel total geordnet sind und jeder einzigartig ist.
Die zu unterstützenden Methoden sind die folgenden:

\begin{itemize}
\item $S$ = MakeSet(): Erschafft eine neue, leere Menge $S$.
\item Insert($i, S$): Fügt das Element $i$ in die Menge $S$ ein.
\item Delete($k, S$): Löscht das Element mit dem Schlüssel $k$ aus der Menge $S$.
\item Find($k, S$): Gibt das Element mit dem Schlüssel $k$ aus der Menge $S$ zurück.
\item $S$ = Join($S_1, i, S_2$): Gib die Menge $S = S_1 \cup {i} \cup S_2$ zurück, wobei:
	\begin{itemize}
	\item für alle Elemente $j \in S_1$ gilt: $k(j) > k(i)$
	\item für alle Elemente $j \in S_2$ gilt: $k(j) < k(i)$
	\end{itemize}
\item $S$ = Merge($S_1, S_2$): Gib die neue Menge $S = S_1 \cup S_2$ zurück, wobei für alle Elemente $i \in S_1$ und $j \in S_2$ gilt, dass $k(i) < k(j)$.
\item $S_1, S_2$ = Split($k, S$): Gib die neuen Mengen $S_1, S_2$, wobei gilt:
	\begin{itemize}
	\item $S_1 = \{j \in S | k(j) < k\}$
	\item $S_2 = \{j \in S | k(j) > k\}$
	\end{itemize}
\end{itemize}

Es liegt nahe, zur Lösung dieses Problems die binären Suchbäume heranzuziehen, da diese die gestellten Voraussetzungen erfüllen.
Es stellt sich jedoch heraus, dass sich bei binären Suchbäumen neue Probleme auftreten, die im nächsten Kapitel genauer erläutert werden.

\section{Probleme bei binären Suchbäumen}
\label{sec:binaryproblems}

Um die Probleme zu sehen, die sich bei binären Suchbäumen anbahnen, müssen wir zuerst die Laufzeiten für die gewünschten Methoden anschauen. Bei dieser Tabelle\ref{tab:binarytree} drückt $h$ die Höhe des Baums aus, und $h_x$ drückt die Höhe von Baum $S_x$ aus.

\begin{table}
\centering
\begin{tabular}{l|c}
	Methode & Laufzeit\\
	\hline
	$S$ = MakeSet() & $O(1)$\\
	Insert$(i, S)$ & $O(h)$ \\
	Delete$(k, S)$ & $O(h)$ \\
	Find$(k, S)$ & $O(h)$ \\
	$S$ = Join($S_1, i, S_2$) & $O(1)$ \\
	$S$ = Merge($S_1, S_2$) & $O(h_1 + h_2)$ \\
	$S_1, S_2$ = Split$(k, S)$ & $O(h)$
\end{tabular}
\caption[Laufzeiten Binärbaum]{Laufzeiten bei der einfachen Implementierung von binären Suchbäumen.} % TODO: evtl noch Erläuterung der Variablen h, h_1 und h_2
\label{tab:binarytree}
\end{table}

Man sieht recht einfach, dass beim binären Suchbaum die Laufzeit vieler Methoden direkt von der Höhe des Baums abhängen.
Dies heißt, dass die amortisierte Laufzeit zwischen best-case und worst-case zwischen $O(log(n))$ und $O(n)$ schwankt.
Da ein Zustand nahe dem worst-case leicht auftreten kann, liegt es nahe, den Suchbaum auf irgendeine Weise zu optimieren, ob es über die Reihenfolge der Insert-Operationen oder über reguläres Rotieren und Reorganisieren der Knoten erreicht wird.

Eine vorherige Optimierung des Baums sieht nach einer guten Lösung für das Problem aus, es bringt jedoch andere, neue Probleme mit sich mit:
Wenn die Datenstruktur die Optimierung übernehmen soll, führt das zu einer erhöhten Komplexität der Implementierung.
Die Methoden werden komplizierter, und es erhöht sich die Chance, dass sich Fehler darin einschleichen.
Weiterhin bringt eine kompliziertere Implementierung eine höhere Laufzeit, auch wenn es sich nicht direkt in der amortisierten Version niederschlägt -- es müssen offensichtlich mehr Operationen ausgeführt werden, um den Baum balanciert zu halten.
Außerdem kann es notwendig sein, dass in der Implementierung zusätzliche Information gespeichert werden muss, die zum Balancieren benötigt wird.
Wenn ein geringer Verbrauch an Speicherplatz wichtig ist, kann dies ebenfalls ein großes Problem darstellen.

Die Lösung für diese Probleme, die in dieser Arbeit näher dargestellt wird, ist Randomisierung.
Sie ist einfach zu implementieren, schnell auszuführen und benötigt keine weitere Speicherung von Balancierungs-Informationen, wenn sie zum Beispiel durch Hash-Funktionen erreicht wird.
Im nächsten Kapitel wird darauf eingegangen, wie eine Datenstruktur durch Einsatz von Randomisierung es sehr wahrscheinlich macht, dass der erstellte Baum zum best-case hin tendiert.

%TODO Passt das so? Vielleicht noch mehr? Keine Ahnung, wie viel der Rest noch wird

\chapter{Treaps}
\label{sec:treaps}

In diesem Kapitel werden die Treaps als Lösung für die vorher genannten Probleme eingeführt.
Am Anfang werden die Treaps grundsätzlich definiert und es wird die Existenz eindeutiger Treaps bewiesen -- eine wichtige Regel für ihre Funktionsweise.
Danach wird die Implementierung der gewünschten Methoden beschrieben, und am Ende wird eine Laufzeitanalyse durchgeführt, um die Vorteile der Treaps zu zeigen.

\section{Grundlagen}
\label{sec:treapsbasics}

Ein binärer Suchbaum ist ein binärer Baum, bei dem die Schlüssel auf der linken Seite kleiner als der der Wurzel sind, und die der rechten Seite größer als die der Wurzel sind.
Ein (Max-)Heap ist eine Datenstruktur, bei der die Objekte total geordnet sind und die Schlüssel der Kinder eines beliebigen Knotens kleiner als die des Elternknotens sind.
Wir benutzen beide Eigenschaften, kombinieren sie und erhalten daraus die Definition für Treaps:

Man nehme einen Binärbaum, bei dem jeder Knoten zwei Zahlen enthält: Einen Schlüssel $k$ und eine Priorität $p$, wobei beide Zahlen aus einer total geordneten Menge stammen, und alle Schlüssel und Prioritäten einzigartig sind. Man erhält somit eine Menge $S = {(k_1, p_1), ... (k_n, p_n)}$, wobei $k_i$ der Schlüssel und $p_i$ die Priorität von Knoten $i$ sind. Dieser Baum wird ein \emph{Treap} genannt, wenn:

\begin{itemize}
\item Alle Schlüssel $k_i$ wie in einem binären Suchbaum sortiert sind
\item Alle Prioritäten $p_i$ wie in einem (Max-)Heap sortiert sind
\end{itemize}

\begin{figure}
\centering
\label{img:exampletreap}
\includegraphics{img/Beispiel_Treap.pdf}
\caption{Beispiel eines Treaps - Schlüssel in rot, Priorität in blau}
\end{figure}


\section{Existenz eindeutiger Treaps}
\label{sec:uniquetreaps}

\begin{satz}
Für jede Menge $S = \{(k_1,p_1), \dots, (k_n, p_n)\}$ von Schlüssel-Priorität-Paaren mit paarweise unterschiedlichen Schlüsseln und Prioritäten existiert genau ein gültiger Treap.
\end{satz}
\begin{beweis}
Es ist trivial, dass dieser Satz für $n = 0$ und $n = 1$ gilt. Betrachten wir jetzt $n \geq 2$, und nehmen an, dass $(k_1, p_1)$ die größte Priorität in $S$ hat. Wir konstruieren einen Treap, indem wir diesen ersten Knoten als die Wurzel für den Treap nehmen, den linken Teilbaum rekursiv aus den Knoten $j$ mit $k_j < k_1$ und den rechten Teilbaum rekursiv aus den Knoten $l$ mit $k_l > k_1$. Es ist weiterhin einfach zu sehen, dass jeder Treap für die Menge $S$ auf diese Weise zusammengesetzt wird.
\end{beweis}

Dieser Satz zeigt, dass die Reihenfolge, in der die Knoten eingefügt werden, beliebig sein kann -- am Ende kommt immer der gleiche Treap heraus. Wenn wir den Baum optimal für eine gegebene Menge an Schlüsseln balancieren wollen, ist es wichtig, die Prioritäten richtig zu wählen. Hier kommt die Randomisierung ins Spiel: Dadurch, dass komplett zufällig gewählte Prioritäten in Richtung einer gleichmäßigen Verteilung tendieren, sind sie ideal für Treaps -- sie sind gleichmäßig und schnell zu erstellen (zum Beispiel durch eine Hashfunktion).

\section{Implementierung}
\label{sec:implementing}
% TODO: erkläre Rotation: - erhält Suchbaumeigenschaft, verändert die Reihenfolge zweier Knoten bzgl. Priorität
%       erkläre nach unten/oben rotieren (normalerweise ist die Definition Rechts-/Linksrotation)

In diesem Kapitel wird das Prinzip der Rotation bei Treaps genauer erläutert und es werden die vorher definierten, gewünschten Methoden in Pseudocode implementiert.

\subsection{Rotation}
\label{sec:rotation}

\subsection{Find}
\label{sec:find}

\subsection{Insert}
\label{sec:insert}

\begin{algorithm}[H]
	\Ein{Knoten $i$, Treap $S$}
		Füge $i$ als Blatt in $S$ ein \;
		\While{$i$.parent.prio $<$ $i$.prio}{
			Rotiere $i$ nach oben \;
		}
	\caption{Insert($i$, $S$)}
\end{algorithm}

\subsection{Delete}
\label{sec:delete}

\begin{algorithm}[H]
	\Ein{Schlüssel $k$, Treap $S$}
		\SetKw{Not}{not}
		\While{\Not istBlatt($i$)}{
			Rotiere $i$ nach unten \;
		}
		Lösche Blatt $i$ aus $S$ \;
	\caption{Delete($k$, $S$)}
\end{algorithm}

\subsection{Join}
\label{sec:join}

\begin{algorithm}[H]
	\Ein{Treaps $S_1, S_2$, Knoten $i$}
	\Aus{Treap $S$}
		\SetKw{Or}{or}
		$i$.left $\gets S_1$ \;
		$i$.right $\gets S_2$ \;
		\While{$i$.prio $<$ $i$.left.prio\\ \Or $i$.prio $<$ $i$.right.prio}{
			Rotiere $i$ nach unten \;
		}
		\Return $i$
	\caption{Join($S_1$, $i$, $S_2$)}
\end{algorithm}

\subsection{Merge}
\label{sec:merge}

\begin{algorithm}[H]
	\Ein{Treaps $S_1, S_2$}
	\Aus{Treap $S$}
		$i$ $\gets$ Find($\infty$, $S_1$) \tcp{Maximum}
		Delete($i$, $S_1$) \;
		\Return Join($S_1$, $i$, $S_2$) \;
	\caption{Merge($S_1$, $S_2$)}
\end{algorithm}

\subsection{Split}
\label{sec:split}

\begin{algorithm}[H]
	\Ein{Treap $S$, Schlüssel $k$}
	\Aus{Treaps $i$.left, $i$.right}
		Delete($k$, $S$) \tcp{falls existent}
		$i \gets (k, \infty)$ \;
		Insert($i$, $S$) \;
		\Return $i$.left, $i$.right;
	\caption{Split($k$, $S$)}
\end{algorithm}

\section{Laufzeitanalyse}
\label{sec:runtime}
Zur Laufzeitanalyse führen wir so genannte \emph{Mulmuley-Spiele} ein.
Diese werden wir analysieren und dadurch die Tiefe von Knoten in einem Treap und die Anzahl der benötigten Rotationen beim Löschen berechnen.
Das genügt, um die Laufzeit aller Operationen zu erhalten.

\subsection{Mulmuley-Spiele}
\label{sec:mulmuley}
Für ein Mulmuley-Spiel werden folgende disjunkte Mengen definiert: % TODO: fordere, dass die Mengen nicht leer sind?
\begin{itemize}
    \item Eine Menge $P = \{P_1, P_2, \dots, P_p\}$ von $p$ \emph{Spielern},
    \item eine Menge $S = \{S_1, S_2, \dots, S_s\}$ von $s$ \emph{Stoppern},
    \item eine Menge $T = \{T_1, T_2, \dots, T_t\}$ von $t$ \emph{Triggern},
    \item eine Menge $B = \{B_1, B_2, \dots, B_b\}$ von $b$ \emph{Zuschauern}.
\end{itemize}
Die Elemente aus $P \cup S$ seien aus einem total geordnetem Universum, und dabei sollen die Spieler kleiner sein als die Stopper.
Die Mengen $T$ und $B$ können beliebig sein.
Bilde eine Menge $X$, indem man mehrere dieser vier Mengen vereinigt und ziehe aus dieser zufällig Elemente ohne Zurücklegen.
Man zählt dabei die Anzahl der Spieler, die größer sind als alle bisher gezogenen Spieler, wobei Stopper ebenfalls als Spieler gezählt werden.
Der Wert eines Spieles entspricht dem erwarteten Wert für diese Anzahl und ist damit abhängig von der Kardinalität der einzelnen Mengen.
In dieser Arbeit werden 3 Varianten der Mulmuley-Spiele vorgestellt:
In Spiel A besteht die Menge $X$ aus den Spielern $P$ und den Zuschauern $B$.
Gezogen wird solange die Menge $X$ nicht leer ist.
In Spiel C besteht die Menge $X$ aus den Spielern $P$, den Stoppern $S$ und den Zuschauern $B$,
Gezogen wird solange bis der erste Stopper gezogen wurde.
In Spiel D besteht die Menge $X$ aus den Spielern $P$, den Triggern $T$ und den Zuschauern $B$.
Gezogen wird solange die Menge $X$ nicht leer ist, allerdings fängt man erst mit dem Zählen an, wenn der erste Trigger gezogen wurde.

Der Wert $A^p$ von Spiel A mit $p$ Spielern beträgt $H_p$. % TODO: \cite{...}
Dabei ist $H_p := \sum_{i=1}^p 1/p$ die p-te harmonische Zahl und es gilt $H_p \in \Theta(\log p)$. % TODO? \cite{...}
Der Wert $C_s^p$ von Spiel B mit $p$ Spielern und $s$ Stoppern beträgt $1 + H_{s+p} - H_s$.
Der  Wert $D_t^p$ von Spiel C mit $p$ Spielern und $t$ Triggern beträgt $H_p + H_t - H_{t+p}$.

\subsection{Laufzeit von Delete}
\label{sec:delruntime}
In diesem Unterabschnitt wird die Laufzeit einer Delete-Operation analysiert.
Dies ist ausreichend, da Insert genau den umgekehrten Ablauf hat wie Delete (\glqq Delete rückwärts\grqq), deshalb die gleiche Laufzeit hat, und die restlichen Operationen mit Hilfe einer konstanten Anzahl von Insert und Delete implementiert werden können. % TODO: beweisen? verlinken?

Die Ausführung von Delete besteht aus 2 Phasen:
Zuerst wird der zu löschende Knoten im Treap gesucht, um dann in der zweiten Phase bis nach unten rotiert zu werden.
Die Laufzeit der ersten Phase hängt linear von der Tiefe des gesuchten Knotens im Baum ab.
Der Knoten wird in der zweiten Phase entlang des linken \emph{Grats} des rechten Teilbaums und des rechten Grats des linken Teilbaums nach unten rotiert.
Der linke/rechte Grat eines Baums ist definiert als der Weg, den man erhält, wenn man von der Wurzel aus immer zum linken/rechten Kindknoten absteigt.
Also ist die Laufzeit der zweiten Phase proportional zur Summe dieser beiden Grate.

Sei $X$ mit $\left|X\right| = n$ die Menge aller Knoten in einem Treap.
Sei $x$ ein Knoten mit Rang $r$, d.h. es gibt $r$ Knoten, deren Schlüssel nicht größer sind, als der von $x$.
Sei $X_\leq$ die Menge aller $r$ Knoten mit Schlüssel kleiner oder gleich $k(x)$.
Analog definiere $X_\geq$.
Zur Analyse beginnen wir mit einem leeren Treap und fügen seine Knoten nach absteigender Priorität ein.
Dies ergibt, wie oben gezeigt, genau den zu analysierenden Treap, wobei Knoten immer nur als Blatt eingefügt werden und keine Rotationen erforderlich sind.
Wie in Randomized Algorithms\cite{TODO} gezeigt, sind die Knoten aus $X_\leq$, die sich entlang des Suchpfades von $x$ befinden, genau die Knoten, die zu dem Zeitpunkt, in dem sie eingefügt werden, die größten aus $X_\leq$ sind.
Möchte man die Anzahl dieser Knoten zählen, spielt man im Prinzip Spiel A mit $P = X_\leq$ und $B = X \setminus X_\leq$.
Damit ergibt sich ein erwartete Anzahl von $H_{\left|P\right|} = H_r$.
Betrachtet man auch die Knoten entlang des Suchpfades aus $X_\geq$, so ergibt sich aus Symmetriegründen eine erwartete Tiefe von $H_r + H_{n-r+1} - 1 \in O(\log n)$ für den Knoten $x$.
% TODO: Bild einfügen
Ähnlich kann man auch für die Anzahl der Rotationen argumentieren.
Hierbei spielt man Spiel D mit $P = X_\leq \setminus \{x\}$, $T = \{x\}$ und $B = X \setminus X_\leq$, das den Erwartungswert $H_{r-1} + H_1 - H_r = 1 - 1/r$ hat.
Für die Gesamtzahl der Rotationen ergibt sich damit erwartet $2 - 1/r - 1/(n-r+1) < 2$.

\chapter{Fazit}
\label{sec:closing}
% Vorteil ggü naiver BT: Balancing -> Laufzeit, gerade, wenn Elemente in auf-/absteigender Reihenfolge eingefügt werden
% Vorteil ggü det. balancierter BT: einfache Implementierung, keine weiteren Informationen/Speicher nötig -> Hashfunktion
% Tabelle mit Laufzeitvergleich
Die in dieser Arbeit vorgestellten Treaps stellen eine gute Option zum Verwalten geordneter Mengen dar.
Gegenüber einfachen binären Suchbäumen haben Treaps den Vorteil, dass sie balanciert werden, was insbesondere dann eine Rolle spielt, wenn Elemente häufig in auf-/absteigender Folge eingefügt werden.
Gegenüber anderen (deterministischen) balancierten Suchbäumen haben Treaps den Vorteil, dass die Implementierung einfach ist.
Außerdem ist es möglich, keinen zusätzlichen Speicherplatz für Balancierungsinformationen zu verwenden, wenn man statt explizite Prioritäten anzugeben auf eine Hashfunktion setzt.
%TODO Nachteil?
Allerdings ist die schnelle Laufzeit bei Treaps nicht garantiert, sondern nur sehr unwahrscheinlich, da es sich um einen Erwartungswert handelt.
Abschließend sei hier nochmal der direkte Vergleich der Laufzeiten dargestellt:

\vspace{1em}
% \begin{table}
% \centering
% \label{tab:treapvsbinarytree}
\begin{tabular}{l | c | c }
        Methode & Treap & Binärbaum\\
        \hline
        Insert$(i, S)$ & $O(\log n)$ & $O(h)$ \\
        Delete$(k, S)$ & $O(\log n)$ & $O(h)$ \\
        Find$(k, S)$ & $O(\log n)$ & $O(h)$ \\
        $S$ = Join($S_1, i, S_2$) & $O(\log n + \log m)$ & $O(1)$ \\
        $S$ = Merge($S_1, S_2$) & $O(\log n + \log m)$ & $O(h_1 + h_2)$ \\
        $S_1, S_2$ = Split$(k, S)$ & $O(\log n + \log m)$ & $O(h)$
        \end{tabular}
% \caption[Laufzeiten Treap vs. Binärbaum]{Laufzeitvergleich zwischen Treaps (Erwartungswert) und binären Suchbäumen (worst case).} % TODO: evtl noch Erläuterung der Variablen h, h_1 und h_2, n, m
% \end{table}

Die Variablen $n$ bzw.\ $m$ stehen hierbei für die Anzahl der Elemente in den jeweiligen Treaps; die Variablen $h$, $h_1$ und $h_2$ geben die Höhe des Binärbaums an.





\bibliographystyle{mybabalpha-fl}
\bibliography{mybib}

\end{document}
